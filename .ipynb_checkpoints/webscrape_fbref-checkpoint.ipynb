{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "cellView": "form",
    "id": "2qYGN7pfk3gK"
   },
   "outputs": [],
   "source": [
    "#@title\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import sys, getopt\n",
    "import csv\n",
    "import time\n",
    "import traceback\n",
    "from functools import wraps\n",
    "from requests.exceptions import HTTPError\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE Most complete data seems to appear after 2018 (eg. Distance for shooting in Championship)\n",
    "\n",
    "# La Liga: 12, Segunda Div: 17, EPL: 9, Championship: 10\n",
    "\n",
    "COMPETITION_MAPPING = {\n",
    "    'ENG': {\n",
    "        9: 'Premier-League',\n",
    "        10: 'Championship'\n",
    "    },\n",
    "    'SPA': {\n",
    "        12: 'La-Liga',\n",
    "        17: 'Segunda-Division'\n",
    "    },\n",
    "    'GER': {\n",
    "        20: 'Bundesliga',\n",
    "        33: '2-Bundesliga'\n",
    "    },\n",
    "    'ITA': {\n",
    "        11: 'Serie-A',\n",
    "        18: 'Serie-B'\n",
    "    },\n",
    "    'FRA': {\n",
    "        13: 'Ligue-1',\n",
    "        60: 'Ligue-2'\n",
    "    }\n",
    "}\n",
    "\n",
    "MAX_RETRIES = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def catch_too_many_requests(func):\n",
    "    @wraps(func)\n",
    "    def wrapper(*args, **kwargs):\n",
    "        for i in range(MAX_RETRIES):\n",
    "            try:\n",
    "                return func(*args, **kwargs)\n",
    "            except HTTPError as http_err:\n",
    "                if hasattr(http_err, 'response') and http_err.response.status_code == 429:\n",
    "                    print(\"Rate limit exceeded.\")\n",
    "                else:\n",
    "                    print(f\"HTTP error occurred: {http_err}\" + \", retrying\" if i < MAX_RETRIES else \"\")\n",
    "    return wrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SeasonData:\n",
    "\n",
    "    def __init__(self, year, country, comp_id):\n",
    "        self.country = country\n",
    "        self.year = year\n",
    "        self.comp_id = comp_id\n",
    "        self.main_url = f\"https://fbref.com/en/comps/{comp_id}/{year}-{year+1}/\"\n",
    "        self.suf_url = f\"/{year}-{year+1}-{COMPETITION_MAPPING[country][comp_id]}-Stats\"\n",
    "\n",
    "        self._all_teams = None\n",
    "\n",
    "    @property\n",
    "    @catch_too_many_requests\n",
    "    def all_teams(self):\n",
    "        global ALL_TEAMS_DICT\n",
    "        if self._all_teams:\n",
    "            pass\n",
    "        elif ALL_TEAMS_DICT[self.country][self.year].get(self.comp_id, None):\n",
    "            self.all_teams = ALL_TEAMS_DICT[self.country][self.year][self.comp_id]\n",
    "            print(f\"Teams found for {self.year}: {self._all_teams}\")\n",
    "        else:\n",
    "            self.all_teams = self.get_all_teams()\n",
    "\n",
    "        return self._all_teams\n",
    "\n",
    "    @all_teams.setter\n",
    "    def all_teams(self, all_teams):\n",
    "        global ALL_TEAMS_DICT\n",
    "        ALL_TEAMS_DICT[self.country][self.year][self.comp_id] = all_teams\n",
    "        self._all_teams = all_teams\n",
    "\n",
    "    def get_all_teams(self):\n",
    "        all_teams = []\n",
    "        res = requests.get(self.main_url)\n",
    "        res.raise_for_status()\n",
    "        comm = re.compile(\"<!--|-->\")\n",
    "        soup = BeautifulSoup(comm.sub(\"\",res.text),'lxml') \n",
    "        all_tables = soup.findAll(\"tbody\")\n",
    "        results_table = all_tables[0]\n",
    "        rows_teams = results_table.find_all('tr')\n",
    "        for row in rows_teams:\n",
    "            team = self.find_team(row)\n",
    "            if not team:\n",
    "                return\n",
    "            all_teams.append(team)\n",
    "        return all_teams\n",
    "            \n",
    "    def find_team(self, row): # promoted or relegated\n",
    "        team_td = row.find('td', {'data-stat': 'team'})\n",
    "        if team_td is not None:\n",
    "            if team_td:\n",
    "                team_name = team_td.find('a').text.strip()\n",
    "                if team_name:\n",
    "                    return team_name\n",
    "                \n",
    "    @catch_too_many_requests\n",
    "    def get_category_table(self, category):\n",
    "        url = self.main_url + category + self.suf_url\n",
    "        res = requests.get(url)\n",
    "        res.raise_for_status()\n",
    "        ## The next two lines get around the issue with comments breaking the parsing.\n",
    "        comm = re.compile(\"<!--|-->\")\n",
    "        soup = BeautifulSoup(comm.sub(\"\",res.text),'lxml')    \n",
    "        all_tables = soup.findAll(\"tbody\")\n",
    "        team_table = all_tables[0]\n",
    "        return team_table\n",
    "\n",
    "class CombinedSeasonData:\n",
    "\n",
    "    categories = ['stats', 'keepers', 'keepersadv', 'shooting', 'passing', 'passing_types', 'gca', 'defense', 'possession', 'misc']\n",
    "    file_path = './data/teams/teams.txt'\n",
    "\n",
    "    def __init__(self, year, country):\n",
    "        self.country = country\n",
    "        self.year = year\n",
    "        self.comp_ids = list(COMPETITION_MAPPING[country].keys())\n",
    "        self.next_season_data = SeasonData(year+1, country, self.comp_ids[0])\n",
    "        self.first_div_data = SeasonData(year, country, self.comp_ids[0])\n",
    "        self.second_div_data = SeasonData(year, country, self.comp_ids[1])\n",
    "\n",
    "        self.df_team = None\n",
    "\n",
    "        self._promoted_teams = None\n",
    "        self._relegated_teams = None\n",
    "\n",
    "    @property\n",
    "    def promoted_teams(self):\n",
    "        if not self._promoted_teams:\n",
    "            self.promoted_teams = list(set(self.next_season_data.all_teams) & set(self.second_div_data.all_teams))\n",
    "\n",
    "        return self._promoted_teams\n",
    "\n",
    "    @promoted_teams.setter\n",
    "    def promoted_teams(self, teams):\n",
    "        self._promoted_teams = teams\n",
    "\n",
    "    @property\n",
    "    def relegated_teams(self):\n",
    "        if not self._relegated_teams:\n",
    "            self.relegated_teams = list(set(self.first_div_data.all_teams) - set(self.next_season_data.all_teams))\n",
    "\n",
    "        return self._relegated_teams\n",
    "\n",
    "    @relegated_teams.setter\n",
    "    def relegated_teams(self, teams):\n",
    "        self._relegated_teams = teams\n",
    "\n",
    "    def get_frame_team(self, team_table, url):\n",
    "        '''This function reads the HTML table rows, row by row, and extracts the individual data. Each row is one team. Any columns within the row that has the 'data-stat' attribute will be extracted. It returns a dictionary whose keys are the columns and values are a list of the column data'''\n",
    "        pre_df_squad = dict()\n",
    "        rows_squad = team_table.find_all('tr')\n",
    "        for row in rows_squad:\n",
    "            if(row.find('th',{\"scope\":\"row\"}) != None) and row.find('th',{\"data-stat\":\"team\"}):\n",
    "                name = row.find('th',{\"data-stat\":\"team\"}).text.strip().encode().decode(\"utf-8\")\n",
    "                if (url == self.second_div_data.main_url and name not in self.promoted_teams) or \\\n",
    "                    (url == self.first_div_data.main_url and name in self.relegated_teams):\n",
    "                    continue\n",
    "                # print(f\"Team found: {name}\")\n",
    "                if 'squad' in pre_df_squad:\n",
    "                    pre_df_squad['squad'].append(name)\n",
    "                else:\n",
    "                    pre_df_squad['squad'] = [name]\n",
    "                for f in [td.get('data-stat') for td in row.find_all('td')]:\n",
    "                    cell = row.find(\"td\",{\"data-stat\": f})\n",
    "                    a = cell.text.strip().encode()\n",
    "                    text=a.decode(\"utf-8\")\n",
    "                    if(text == ''):\n",
    "                        text = None\n",
    "                    if f in pre_df_squad:\n",
    "                        pre_df_squad[f].append(text)\n",
    "                    else:\n",
    "                        pre_df_squad[f] = [text]\n",
    "        df_squad = pd.DataFrame.from_dict(pre_df_squad)\n",
    "    #     print(df_squad.columns) # print columns for each table\n",
    "        return df_squad\n",
    "\n",
    "    def frame_for_category_team(self, category):\n",
    "        '''Sends 2 requests to retrieve data for the specified category from the 1st division and the 2nd division. Afterwards, the 2 tables are concatenated vertically. Each row is each team that appears in the following season'''\n",
    "        df_divs_array = []\n",
    "        for div_data in [self.first_div_data, self.second_div_data]:\n",
    "            team_table = div_data.get_category_table(category)\n",
    "            df_team = self.get_frame_team(team_table, div_data.main_url)\n",
    "            df_divs_array.append(df_team)\n",
    "        \n",
    "        return pd.concat(df_divs_array)\n",
    "\n",
    "    #Function to get team-wise data accross all categories as mentioned above\n",
    "    def get_team_data(self):\n",
    "        '''This is the 'main' function that will retrieve all data for each category for each team within the current season that appears in the following season. The categories are concatenated horizonatally such that len(rows) = len(teams)'''\n",
    "        df_array = []\n",
    "        for category in self.categories:\n",
    "            df_array.append(self.frame_for_category_team(category))\n",
    "            time.sleep(5)\n",
    "        df = pd.concat(df_array, axis=1)\n",
    "        self.df_team = df.loc[:,~df.columns.duplicated()].copy()\n",
    "        self.df_team['season_start_year'] = self.year\n",
    "        self.df_team['season_end_year'] = self.year + 1\n",
    "    \n",
    "    def save_to_csv(self):\n",
    "        try:\n",
    "            self.df_team.to_csv(f\"./data/teams/raw/{self.country}{self.year}_teams_for_{self.year+1}.csv\",index=False)\n",
    "            print(f\"Saved successfully ({self.year})\")\n",
    "        except Exception as e:\n",
    "            print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 847
    },
    "id": "5e_hIIV3mYP-",
    "outputId": "613375c0-e015-4e4e-f18d-ff8778e13a9a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved successfully (2018)\n",
      "Saved successfully (2019)\n",
      "Saved successfully (2020)\n",
      "Saved successfully (2021)\n",
      "Saved successfully (2022)\n",
      "Saved successfully (2018)\n",
      "Saved successfully (2019)\n",
      "Saved successfully (2020)\n",
      "Saved successfully (2021)\n",
      "Saved successfully (2022)\n",
      "Saved successfully (2018)\n",
      "Saved successfully (2019)\n",
      "Saved successfully (2020)\n",
      "Saved successfully (2021)\n",
      "Saved successfully (2022)\n",
      "Saved successfully (2018)\n",
      "Saved successfully (2019)\n",
      "Saved successfully (2020)\n",
      "Saved successfully (2021)\n",
      "Saved successfully (2022)\n",
      "Saved successfully (2018)\n",
      "Saved successfully (2019)\n",
      "Saved successfully (2020)\n",
      "Saved successfully (2021)\n",
      "Saved successfully (2022)\n"
     ]
    }
   ],
   "source": [
    "#Go to the 'Standard stats' page of the league\n",
    "#For Premier League 2020/21, the link is this: https://fbref.com/en/comps/9/stats/Premier-League-Stats\n",
    "#Remove the 'stats', and pass the first and third part of the link as parameters like below\n",
    "\n",
    "# NOTE: may need to add delays between requests to prevent 429 (gettting blocked by fbref)\n",
    "\n",
    "ALL_TEAMS_DICT = {} # when checking the current season teams, the previous iteration checking for next season teams would have found it already\n",
    "\n",
    "from requests.exceptions import HTTPError\n",
    "\n",
    "for country in COMPETITION_MAPPING.keys():\n",
    "    ALL_TEAMS_DICT[country] = {}\n",
    "    for year in range(2018, 2023):\n",
    "        ALL_TEAMS_DICT[country][year] = {}\n",
    "        ALL_TEAMS_DICT[country][year+1] = {}\n",
    "\n",
    "        season_data = CombinedSeasonData(year, country)\n",
    "\n",
    "        season_data.get_team_data() # sets self.df_team\n",
    "\n",
    "        season_data.save_to_csv()"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "Scrape_FBref.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
